{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Assignment8.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPAxMzijpT3EvTi6dPB30Gl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prathamagarwal296/MLDM-Assignment-10/blob/master/Assignment8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjNgm364PY_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from seaborn import heatmap\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, make_scorer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import NearMiss\n",
        "\n",
        "# Reading the data-set\n",
        "data_set = pd.read_csv(\"WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n",
        "print(data_set.head())\n",
        "\n",
        "# Check For Null Values\n",
        "print(data_set.isnull().sum())\n",
        "\n",
        "# For Duplicate Columns if present\n",
        "print(len(data_set))\n",
        "print(len(data_set.drop_duplicates()))\n",
        "\n",
        "# Checking For Unique Values\n",
        "print(data_set[\"BusinessTravel\"].unique())\n",
        "print(data_set[\"EmployeeCount\"].unique())\n",
        "print(data_set[\"StandardHours\"].unique())\n",
        "print(data_set[\"Over18\"].unique())\n",
        "\n",
        "# Removing Columns\n",
        "columns_to_remove = [\"EmployeeCount\", \"EmployeeNumber\", \"StandardHours\", \"Over18\"]\n",
        "data_set.drop(columns=columns_to_remove, inplace=True)\n",
        "\n",
        "\"\"\"Data-Set Visualization\"\"\"\n",
        "\n",
        "# Variations Of Data In respective columns\n",
        "data_set.hist()\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(20, 20)\n",
        "\n",
        "# Correlation\n",
        "cor_mat = data_set.corr()\n",
        "mask = np.array(cor_mat)\n",
        "# Taking the lower triangle\n",
        "mask[np.tril_indices_from(mask)] = False\n",
        "heatmap(data_set.corr(), annot=True, mask=mask, cbar=True)\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(20, 10)\n",
        "plt.show()\n",
        "\n",
        "# KDE\n",
        "sns.kdeplot(data_set['Age'], color='purple')\n",
        "plt.show()\n",
        "\n",
        "sns.kdeplot(data_set['TotalWorkingYears'], color='black')\n",
        "plt.show()\n",
        "\n",
        "sns.kdeplot(data_set['YearsAtCompany'], color='purple')\n",
        "plt.show()\n",
        "\n",
        "sns.kdeplot(data_set['PercentSalaryHike'], color='purple')\n",
        "plt.show()\n",
        "\n",
        "# Variations\n",
        "plt.hist(data_set['Attrition'], bins=3)\n",
        "plt.show()\n",
        "\n",
        "plt.hist(data_set['BusinessTravel'], bins=5)\n",
        "plt.show()\n",
        "\n",
        "plt.hist(data_set['OverTime'], bins=3)\n",
        "plt.show()\n",
        "\n",
        "plt.hist(data_set['Department'], bins=5)\n",
        "plt.show()\n",
        "\n",
        "plt.hist(data_set['EducationField'], bins=11)\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(10, 5)\n",
        "\n",
        "plt.hist(data_set['Gender'], bins=3)\n",
        "plt.show()\n",
        "\n",
        "plt.hist(data_set['JobRole'], bins=17)\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(20, 10)\n",
        "\n",
        "# Encoding Columns\n",
        "data_set.loc[:, \"Attrition\"] = LabelEncoder().fit_transform(data_set.loc[:, \"Attrition\"])\n",
        "data_set.loc[:, \"BusinessTravel\"] = LabelEncoder().fit_transform(data_set.loc[:, \"BusinessTravel\"])\n",
        "data_set.loc[:, \"Department\"] = LabelEncoder().fit_transform(data_set.loc[:, \"Department\"])\n",
        "data_set.loc[:, \"EducationField\"] = LabelEncoder().fit_transform(data_set.loc[:, \"EducationField\"])\n",
        "data_set.loc[:, \"Gender\"] = LabelEncoder().fit_transform(data_set.loc[:, \"Gender\"])\n",
        "data_set.loc[:, \"JobRole\"] = LabelEncoder().fit_transform(data_set.loc[:, \"JobRole\"])\n",
        "data_set.loc[:, \"MaritalStatus\"] = LabelEncoder().fit_transform(data_set.loc[:, \"MaritalStatus\"])\n",
        "data_set.loc[:, \"OverTime\"] = LabelEncoder().fit_transform(data_set.loc[:, \"OverTime\"])\n",
        "\n",
        "# Data-Set Columns\n",
        "print(data_set.columns)\n",
        "\n",
        "# Data-Set Extract\n",
        "X = data_set.iloc[:, [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
        "                      18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]].values\n",
        "y = data_set.iloc[:, 1].values\n",
        "\n",
        "# Scaling The Values\n",
        "sc_X = StandardScaler()\n",
        "X_scaled = sc_X.fit_transform(X)\n",
        "X_scaled = np.append(arr=np.ones((len(X_scaled), 1)).astype(float), values=X_scaled, axis=1)\n",
        "\n",
        "# Splitting Data-Set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=0, test_size=0.4)\n",
        "\n",
        "# OverSampling\n",
        "Smote = SMOTE(random_state=0)\n",
        "X_train_Over, y_train_Over = Smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# UnderSampling\n",
        "NearMiss = NearMiss()\n",
        "X_train_Under, y_train_Under = NearMiss.fit_sample(X_train, y_train)\n",
        "\n",
        "# Scoring\n",
        "scoring = make_scorer(balanced_accuracy_score)\n",
        "\n",
        "# Logistic Regression\n",
        "def Logistic_Grid():\n",
        "    parameter = [{'penalty': [\"l2\", \"none\"]}]\n",
        "    return parameter\n",
        "\n",
        "\n",
        "def Decision_Grid():\n",
        "    parameter = [{'criterion': [\"gini\", \"entropy\"]}]\n",
        "    return parameter\n",
        "\n",
        "\n",
        "def Random_Grid():\n",
        "    parameter = [{'criterion': [\"gini\", \"entropy\"],\n",
        "                  'n_estimators': [100, 200, 300, 400, 500]}]\n",
        "    return parameter\n",
        "\n",
        "\n",
        "def K_NN_Grid():\n",
        "    parameter = [{'n_neighbors': [3, 5, 7]}]\n",
        "    return parameter\n",
        "\n",
        "\n",
        "def SVM_Grid():\n",
        "    parameter = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
        "                 {'C': [1, 10, 100, 1000], 'kernel': ['rbf'],\n",
        "                  'gamma': [0.1, 0.001, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
        "    return parameter\n",
        "\n",
        "\n",
        "def Scores_And_GridSearch(string, value):\n",
        "    global parameters\n",
        "    if value == 0:\n",
        "        x, y_ = X_train, y_train\n",
        "    elif value == 1:\n",
        "        x, y_ = X_train_Over, y_train_Over\n",
        "    else:\n",
        "        x, y_ = X_train_Under, y_train_Under\n",
        "\n",
        "    if string == \"Logistic\":\n",
        "        parameters = Logistic_Grid()\n",
        "        grid_search = GridSearchCV(estimator=classifier,\n",
        "                                   param_grid=parameters,\n",
        "                                   scoring=scoring,\n",
        "                                   cv=10,\n",
        "                                   n_jobs=-1)\n",
        "\n",
        "        grid_search = grid_search.fit(x, y_)\n",
        "        best_parameters = grid_search.best_params_\n",
        "        print(best_parameters)\n",
        "    elif string == \"Decision\":\n",
        "        parameters = Decision_Grid()\n",
        "        grid_search = GridSearchCV(estimator=classifier,\n",
        "                                   param_grid=parameters,\n",
        "                                   scoring=scoring,\n",
        "                                   cv=10,\n",
        "                                   n_jobs=-1)\n",
        "\n",
        "        grid_search = grid_search.fit(x, y_)\n",
        "        best_parameters = grid_search.best_params_\n",
        "        print(best_parameters)\n",
        "    elif string == \"Random\":\n",
        "        parameters = Random_Grid()\n",
        "        grid_search = GridSearchCV(estimator=classifier,\n",
        "                                   param_grid=parameters,\n",
        "                                   scoring=scoring,\n",
        "                                   cv=10,\n",
        "                                   n_jobs=-1)\n",
        "\n",
        "        grid_search = grid_search.fit(x, y_)\n",
        "        best_parameters = grid_search.best_params_\n",
        "        print(best_parameters)\n",
        "    elif string == \"K\":\n",
        "        parameters = K_NN_Grid()\n",
        "        grid_search = GridSearchCV(estimator=classifier,\n",
        "                                   param_grid=parameters,\n",
        "                                   scoring=scoring,\n",
        "                                   cv=10,\n",
        "                                   n_jobs=-1)\n",
        "\n",
        "        grid_search = grid_search.fit(x, y_)\n",
        "        best_parameters = grid_search.best_params_\n",
        "        print(best_parameters)\n",
        "    elif string == \"SVM\":\n",
        "        parameters = SVM_Grid()\n",
        "        grid_search = GridSearchCV(estimator=classifier,\n",
        "                                   param_grid=parameters,\n",
        "                                   scoring=scoring,\n",
        "                                   cv=10,\n",
        "                                   n_jobs=-1)\n",
        "\n",
        "        grid_search = grid_search.fit(x, y_)\n",
        "        best_parameters = grid_search.best_params_\n",
        "        print(best_parameters)\n",
        "\n",
        "\n",
        "def score_calculator():\n",
        "    print(\"Accuracy :\", balanced_accuracy_score(y_test, predictions))\n",
        "    print(\"Confusion metric :\", confusion_matrix(y_test, predictions))\n",
        "    print(\"f1_Score :\", f1_score(y_test, predictions))\n",
        "    print(\"Precision :\", precision_score(y_test, predictions))\n",
        "    print(\"Recall :\", recall_score(y_test, predictions))\n",
        "\n",
        "\n",
        "# Logistic Classifier (Original Sample)\n",
        "print(\"Logistic Classifier (Original Sample)\")\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"Logistic\", 0)\n",
        "\n",
        "# Logistic Classifier (Original Sample) With Parameter Tuned\n",
        "print(\"Logistic Classifier (Original Sample) With Tuned Parameter\")\n",
        "classifier = LogisticRegression(penalty='none')\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "\n",
        "# Logistic Classifier (Over Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"Logistic Classifier (Over Sampling)\")\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"Logistic\", 1)\n",
        "\n",
        "# Logistic Classifier (Over Sampling) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Logistic Classifier (Over Sampling) With Parameter Tuned\")\n",
        "classifier = LogisticRegression(penalty='l2')\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "\n",
        "# Logistic Classifier (Under Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\" Logistic Classifier (Under Sampling)\")\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"Logistic\", 2)\n",
        "\n",
        "# Logistic Classifier (Under Sampling) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\" Logistic Classifier (Under Sampling) With Parameter Tuned\")\n",
        "classifier = LogisticRegression(penalty='l2')\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "\n",
        "# Naive Bayes Classifier (Original Sample)\n",
        "print(\"=\" * 40)\n",
        "print(\"Naive Bayes Classifier (Original Sample)\")\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "\n",
        "# Naive Bayes Classifier (Over Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"Naive Bayes Classifier (Over Sampling)\")\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "\n",
        "# Naive Bayes Classifier (Under Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"Naive Bayes Classifier (Under Sampling)\")\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "\n",
        "# Decision Tree Classifier (Original Sample)\n",
        "print(\"=\" * 40)\n",
        "print(\"Decision Tree Classifier (Original Sample)\")\n",
        "classifier = DecisionTreeClassifier(random_state=0)\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"Decision\", 0)\n",
        "\n",
        "# Decision Tree Classifier (Original Sample) with parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Decision Tree Classifier (Original Sample) With Parameter Tuned\")\n",
        "classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "\n",
        "# Decision Tree Classifier (Over Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"Decision Tree Classifier (Over Sampling)\")\n",
        "classifier = DecisionTreeClassifier(random_state=0)\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"Decision\", 1)\n",
        "\n",
        "# Decision Tree Classifier (Over Sampling) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Decision Tree Classifier (Over Sampling), With Parameter Tuned\")\n",
        "classifier = DecisionTreeClassifier(random_state=0, criterion='gini')\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "\n",
        "# Decision Tree Classifier (Under Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"Decision Tree Classifier (Under Sampling)\")\n",
        "classifier = DecisionTreeClassifier(random_state=0)\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"Decision\", 2)\n",
        "\n",
        "# Decision Tree Classifier (Under Sampling) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Decision Tree Classifier (Under Sampling) With Parameter Tuned\")\n",
        "classifier = DecisionTreeClassifier(random_state=0, criterion='entropy')\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "\n",
        "# Random Forest Classifier (Original Sample)\n",
        "print(\"=\" * 40)\n",
        "print(\"Random Forest Classifier (Original Sample)\")\n",
        "classifier = RandomForestClassifier()\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"Random\", 0)\n",
        "\n",
        "# Random Forest Classifier (Original Sample) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Random Forest Classifier (Original Sample) With Parameter Tuned\")\n",
        "classifier = RandomForestClassifier(criterion='gini', n_estimators=300)\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "\n",
        "# Random Forest Classifier (Over Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"Random Forest Classifier (Over Sampling)\")\n",
        "classifier = RandomForestClassifier()\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"Random\", 1)\n",
        "\n",
        "# Random Forest Classifier (Over Sampling) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Random Forest Classifier (Over Sampling) With Parameter Tuned\")\n",
        "classifier = RandomForestClassifier(criterion='entropy', n_estimators=100)\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "\n",
        "# Random Forest Classifier (Under Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"Random Forest Classifier (Under Sampling)\")\n",
        "classifier = RandomForestClassifier()\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"Random\", 2)\n",
        "\n",
        "# Random Forest Classifier (Under Sampling) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Random Forest Classifier (Under Sampling) With Parameter Tuned\")\n",
        "classifier = RandomForestClassifier(criterion='entropy', n_estimators=200)\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "\n",
        "# K-NN Classifier (Original Data)\n",
        "print(\"=\" * 40)\n",
        "print(\"K-NN Classifier (Original Data)\")\n",
        "classifier = KNeighborsClassifier()\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"K\", 0)\n",
        "\n",
        "# K-NN Classifier (Original Data) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"K-NN Classifier (Original Data) With Parameter Tuned\")\n",
        "classifier = KNeighborsClassifier(n_neighbors=3)\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "\n",
        "# K-NN Classifier (Over Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"K-NN Classifier (Over Sampling)\")\n",
        "classifier = KNeighborsClassifier()\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"K\", 1)\n",
        "\n",
        "# K-NN Classifier (Over Sampling) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"K-NN Classifier (Over Sampling) with Parameter Tuned\")\n",
        "classifier = KNeighborsClassifier(n_neighbors=3)\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "\n",
        "# K-NN Classifier (Under Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"K-NN Classifier (Under Sampling)\")\n",
        "classifier = KNeighborsClassifier()\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "\n",
        "# Support Vector Classifier (Original Sample)\n",
        "print(\"=\" * 40)\n",
        "print(\"Support Vector Classifier (Original Sample)\")\n",
        "classifier = SVC(kernel='rbf')\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"SVM\", 0)\n",
        "\n",
        "# Support Vector Classifier (Original Sample) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Support Vector Classifier (Original Sample) With Parameter Tuned\")\n",
        "classifier = SVC(kernel='rbf', C=1000, gamma=0.01)\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "\n",
        "# Support Vector Classifier (Over Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"Support Vector Classifier (Over Sampling)\")\n",
        "classifier = SVC(kernel='rbf')\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"SVM\", 1)\n",
        "\n",
        "# Support Vector Classifier (Over Sampling) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Support Vector Classifier (Over Sampling) With Parameter Tuned\")\n",
        "classifier = SVC(kernel='rbf', C=10, gamma=0.2)\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "\n",
        "# Support Vector Classifier (Under Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"Support Vector Classifier (Under Sampling)\")\n",
        "classifier = SVC(kernel='rbf')\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"SVM\", 2)\n",
        "\n",
        "# Support Vector Classifier (Under Sampling) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Support Vector Classifier (Under Sampling) With Parameter Tuned\")\n",
        "classifier = SVC(kernel='rbf', C=1, gamma=0.1)\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"SVM\", 2)\n",
        "\n",
        "\"\"\"After Performing parameter tuning, over-sampling and under-sampling we came to a conclusing for choosing\n",
        "a model with good recall_score and a good balanced_accuracy score.\n",
        "We are choosing balanced accuracy score because it is a measure of recall of positive class + recall of negative class \n",
        "and it outperforms f1_score when positives >> negatives\n",
        "We according to the results got, the best model is Logistic Classifier with oversampling\n",
        "Cause we are getting a good balanced accuracy around 76%\n",
        "And a recall about 76.5%\"\"\"\n",
        "\n",
        "print(\"=\" * 40)\n",
        "print(\"Logistic Classifier\")\n",
        "classifier = LogisticRegression(penalty='l2')\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}