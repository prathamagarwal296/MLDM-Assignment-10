{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment8.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOJXsxk5yI/hvs+eUhe90pI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prathamagarwal296/MLDM-Assignments/blob/master/Assignment8_PrathamAgarwal_1700158C202.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU_6v7J8eHKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from seaborn import heatmap\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, make_scorer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import NearMiss\n",
        "In [144]:\n",
        "# Reading the data-set\n",
        "data_set = pd.read_csv(\"/content/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n",
        "print(data_set.head())\n",
        "   Age Attrition  ... YearsSinceLastPromotion  YearsWithCurrManager\n",
        "0   41       Yes  ...                       0                     5\n",
        "1   49        No  ...                       1                     7\n",
        "2   37       Yes  ...                       0                     0\n",
        "3   33        No  ...                       3                     0\n",
        "4   27        No  ...                       2                     2\n",
        "\n",
        "[5 rows x 35 columns]\n",
        "In [145]:\n",
        "# Check For Null Values\n",
        "print(data_set.isnull().sum())\n",
        "Age                         0\n",
        "Attrition                   0\n",
        "BusinessTravel              0\n",
        "DailyRate                   0\n",
        "Department                  0\n",
        "DistanceFromHome            0\n",
        "Education                   0\n",
        "EducationField              0\n",
        "EmployeeCount               0\n",
        "EmployeeNumber              0\n",
        "EnvironmentSatisfaction     0\n",
        "Gender                      0\n",
        "HourlyRate                  0\n",
        "JobInvolvement              0\n",
        "JobLevel                    0\n",
        "JobRole                     0\n",
        "JobSatisfaction             0\n",
        "MaritalStatus               0\n",
        "MonthlyIncome               0\n",
        "MonthlyRate                 0\n",
        "NumCompaniesWorked          0\n",
        "Over18                      0\n",
        "OverTime                    0\n",
        "PercentSalaryHike           0\n",
        "PerformanceRating           0\n",
        "RelationshipSatisfaction    0\n",
        "StandardHours               0\n",
        "StockOptionLevel            0\n",
        "TotalWorkingYears           0\n",
        "TrainingTimesLastYear       0\n",
        "WorkLifeBalance             0\n",
        "YearsAtCompany              0\n",
        "YearsInCurrentRole          0\n",
        "YearsSinceLastPromotion     0\n",
        "YearsWithCurrManager        0\n",
        "dtype: int64\n",
        "In [146]:\n",
        "# For Duplicate Columns if present\n",
        "print(len(data_set))\n",
        "print(len(data_set.drop_duplicates()))\n",
        "1470\n",
        "1470\n",
        "In [147]:\n",
        "# Checking For Unique Values\n",
        "print(data_set[\"BusinessTravel\"].unique())\n",
        "print(data_set[\"EmployeeCount\"].unique())\n",
        "print(data_set[\"StandardHours\"].unique())\n",
        "print(data_set[\"Over18\"].unique())\n",
        "['Travel_Rarely' 'Travel_Frequently' 'Non-Travel']\n",
        "[1]\n",
        "[80]\n",
        "['Y']\n",
        "In [0]:\n",
        "# Removing Columns\n",
        "columns_to_remove = [\"EmployeeCount\", \"EmployeeNumber\", \"StandardHours\", \"Over18\"]\n",
        "data_set.drop(columns=columns_to_remove, inplace=True)\n",
        "Data-Set Visualization\n",
        "In [149]:\n",
        "# Variations Of Data In respective columns\n",
        "data_set.hist()\n",
        "fig=plt.gcf()\n",
        "fig.set_size_inches(20,20)\n",
        "\n",
        "In [150]:\n",
        "# Correlation\n",
        "cor_mat = data_set.corr()\n",
        "mask = np.array(cor_mat)\n",
        "# Taking the lower triangle\n",
        "mask[np.tril_indices_from(mask)] = False\n",
        "heatmap(data_set.corr(), annot=True, mask=mask, cbar=True)\n",
        "fig=plt.gcf()\n",
        "fig.set_size_inches(20,10)\n",
        "\n",
        "In [151]:\n",
        "# KDE\n",
        "sns.kdeplot(data_set['Age'], color='purple')\n",
        "plt.show()\n",
        "\n",
        "In [152]:\n",
        "sns.kdeplot(data_set['TotalWorkingYears'], color='black')\n",
        "plt.show()\n",
        "\n",
        "In [153]:\n",
        "sns.kdeplot(data_set['YearsAtCompany'], color='purple')\n",
        "plt.show()\n",
        "\n",
        "In [154]:\n",
        "sns.kdeplot(data_set['PercentSalaryHike'], color='purple')\n",
        "plt.show()\n",
        "\n",
        "In [155]:\n",
        "# Variations\n",
        "plt.hist(data_set['Attrition'], bins=3)\n",
        "plt.show()\n",
        "\n",
        "In [156]:\n",
        "plt.hist(data_set['BusinessTravel'], bins=5)\n",
        "plt.show()\n",
        "\n",
        "In [157]:\n",
        "plt.hist(data_set['OverTime'], bins=3)\n",
        "plt.show()\n",
        "\n",
        "In [158]:\n",
        "plt.hist(data_set['Department'], bins=5)\n",
        "plt.show()\n",
        "\n",
        "In [159]:\n",
        "plt.hist(data_set['EducationField'], bins=11)\n",
        "fig=plt.gcf()\n",
        "fig.set_size_inches(10,5)\n",
        "\n",
        "In [160]:\n",
        "plt.hist(data_set['Gender'], bins=3)\n",
        "plt.show()\n",
        "\n",
        "In [161]:\n",
        "plt.hist(data_set['JobRole'], bins=17)\n",
        "fig=plt.gcf()\n",
        "fig.set_size_inches(20, 10)\n",
        "\n",
        "In [0]:\n",
        "# Encoding Columns\n",
        "data_set.loc[:, \"Attrition\"] = LabelEncoder().fit_transform(data_set.loc[:, \"Attrition\"])\n",
        "data_set.loc[:, \"BusinessTravel\"] = LabelEncoder().fit_transform(data_set.loc[:, \"BusinessTravel\"])\n",
        "data_set.loc[:, \"Department\"] = LabelEncoder().fit_transform(data_set.loc[:, \"Department\"])\n",
        "data_set.loc[:, \"EducationField\"] = LabelEncoder().fit_transform(data_set.loc[:, \"EducationField\"])\n",
        "data_set.loc[:, \"Gender\"] = LabelEncoder().fit_transform(data_set.loc[:, \"Gender\"])\n",
        "data_set.loc[:, \"JobRole\"] = LabelEncoder().fit_transform(data_set.loc[:, \"JobRole\"])\n",
        "data_set.loc[:, \"MaritalStatus\"] = LabelEncoder().fit_transform(data_set.loc[:, \"MaritalStatus\"])\n",
        "data_set.loc[:, \"OverTime\"] = LabelEncoder().fit_transform(data_set.loc[:, \"OverTime\"])\n",
        "In [163]:\n",
        "# Data-Set Columns\n",
        "print(data_set.columns)\n",
        "Index(['Age', 'Attrition', 'BusinessTravel', 'DailyRate', 'Department',\n",
        "       'DistanceFromHome', 'Education', 'EducationField',\n",
        "       'EnvironmentSatisfaction', 'Gender', 'HourlyRate', 'JobInvolvement',\n",
        "       'JobLevel', 'JobRole', 'JobSatisfaction', 'MaritalStatus',\n",
        "       'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'OverTime',\n",
        "       'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction',\n",
        "       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n",
        "       'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole',\n",
        "       'YearsSinceLastPromotion', 'YearsWithCurrManager'],\n",
        "      dtype='object')\n",
        "In [0]:\n",
        "# Data-Set Extract\n",
        "X = data_set.iloc[:, [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
        "                      18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]].values\n",
        "y = data_set.iloc[:, 1].values\n",
        "In [0]:\n",
        "# Scaling The Values\n",
        "sc_X = StandardScaler()\n",
        "X_scaled = sc_X.fit_transform(X)\n",
        "X_scaled = np.append(arr=np.ones((len(X_scaled), 1)).astype(float), values=X_scaled, axis=1)\n",
        "In [0]:\n",
        "# Splitting Data-Set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=0, test_size=0.4)\n",
        "In [167]:\n",
        "# OverSampling\n",
        "Smote = SMOTE(random_state=0)\n",
        "X_train_Over, y_train_Over = Smote.fit_resample(X_train, y_train)\n",
        "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
        "  warnings.warn(msg, category=FutureWarning)\n",
        "In [168]:\n",
        "# UnderSampling\n",
        "NearMiss = NearMiss()\n",
        "X_train_Under, y_train_Under = NearMiss.fit_sample(X_train, y_train)\n",
        "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
        "  warnings.warn(msg, category=FutureWarning)\n",
        "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
        "  warnings.warn(msg, category=FutureWarning)\n",
        "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
        "  warnings.warn(msg, category=FutureWarning)\n",
        "If need any other scoring technique for parameter tuning change the scoring metric below\n",
        "In [0]:\n",
        "# Scoring\n",
        "scoring = make_scorer(balanced_accuracy_score)\n",
        "In [0]:\n",
        "# Logistic Regression\n",
        "def Logistic_Grid():\n",
        "    parameter = [{'penalty': [\"l2\", \"none\"]}]\n",
        "    return parameter\n",
        "In [0]:\n",
        "def Decision_Grid():\n",
        "    parameter = [{'criterion': [\"gini\", \"entropy\"]}]\n",
        "    return parameter\n",
        "In [0]:\n",
        "def Random_Grid():\n",
        "    parameter = [{'criterion': [\"gini\", \"entropy\"],\n",
        "                  'n_estimators': [100, 200, 300, 400, 500]}]\n",
        "    return parameter\n",
        "In [0]:\n",
        "def K_NN_Grid():\n",
        "    parameter = [{'n_neighbors': [3, 5, 7]}]\n",
        "    return parameter\n",
        "In [0]:\n",
        "def SVM_Grid():\n",
        "    parameter = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
        "                 {'C': [1, 10, 100, 1000], 'kernel': ['rbf'],\n",
        "                  'gamma': [0.1, 0.001, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
        "    return parameter\n",
        "In [0]:\n",
        "def Scores_And_GridSearch(string, value):\n",
        "    global parameters\n",
        "    if value == 0:\n",
        "        x, y_ = X_train, y_train\n",
        "    elif value == 1:\n",
        "        x, y_ = X_train_Over, y_train_Over\n",
        "    else:\n",
        "        x, y_ = X_train_Under, y_train_Under\n",
        "\n",
        "    if string == \"Logistic\":\n",
        "        parameters = Logistic_Grid()\n",
        "        grid_search = GridSearchCV(estimator=classifier,\n",
        "                                   param_grid=parameters,\n",
        "                                   scoring=scoring,\n",
        "                                   cv=10,\n",
        "                                   n_jobs=-1)\n",
        "\n",
        "        grid_search = grid_search.fit(x, y_)\n",
        "        best_parameters = grid_search.best_params_\n",
        "        print(best_parameters)\n",
        "    elif string == \"Decision\":\n",
        "        parameters = Decision_Grid()\n",
        "        grid_search = GridSearchCV(estimator=classifier,\n",
        "                                   param_grid=parameters,\n",
        "                                   scoring=scoring,\n",
        "                                   cv=10,\n",
        "                                   n_jobs=-1)\n",
        "\n",
        "        grid_search = grid_search.fit(x, y_)\n",
        "        best_parameters = grid_search.best_params_\n",
        "        print(best_parameters)\n",
        "    elif string == \"Random\":\n",
        "        parameters = Random_Grid()\n",
        "        grid_search = GridSearchCV(estimator=classifier,\n",
        "                                   param_grid=parameters,\n",
        "                                   scoring=scoring,\n",
        "                                   cv=10,\n",
        "                                   n_jobs=-1)\n",
        "\n",
        "        grid_search = grid_search.fit(x, y_)\n",
        "        best_parameters = grid_search.best_params_\n",
        "        print(best_parameters)\n",
        "    elif string == \"K\":\n",
        "        parameters = K_NN_Grid()\n",
        "        grid_search = GridSearchCV(estimator=classifier,\n",
        "                                   param_grid=parameters,\n",
        "                                   scoring=scoring,\n",
        "                                   cv=10,\n",
        "                                   n_jobs=-1)\n",
        "\n",
        "        grid_search = grid_search.fit(x, y_)\n",
        "        best_parameters = grid_search.best_params_\n",
        "        print(best_parameters)\n",
        "    elif string == \"SVM\":\n",
        "        parameters = SVM_Grid()\n",
        "        grid_search = GridSearchCV(estimator=classifier,\n",
        "                                   param_grid=parameters,\n",
        "                                   scoring=scoring,\n",
        "                                   cv=10,\n",
        "                                   n_jobs=-1)\n",
        "\n",
        "        grid_search = grid_search.fit(x, y_)\n",
        "        best_parameters = grid_search.best_params_\n",
        "        print(best_parameters)\n",
        "In [0]:\n",
        "def score_calculator():\n",
        "    print(\"Accuracy :\", balanced_accuracy_score(y_test, predictions))\n",
        "    print(\"Confusion metric :\", confusion_matrix(y_test, predictions))\n",
        "    print(\"f1_Score :\", f1_score(y_test, predictions))\n",
        "    print(\"Precision :\", precision_score(y_test, predictions))\n",
        "    print(\"Recall :\", recall_score(y_test, predictions))\n",
        "In [177]:\n",
        "# Logistic Classifier (Original Sample)\n",
        "print(\"Logistic Classifier (Original Sample)\")\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"Logistic\", 0)\n",
        "Logistic Classifier (Original Sample)\n",
        "Accuracy : 0.6545454545454545\n",
        "Confusion metric : [[483  12]\n",
        " [ 62  31]]\n",
        "f1_Score : 0.4558823529411765\n",
        "Precision : 0.7209302325581395\n",
        "Recall : 0.3333333333333333\n",
        "{'penalty': 'none'}\n",
        "In [178]:\n",
        "# Logistic Classifier (Original Sample) With Parameter Tuned\n",
        "print(\"Logistic Classifier (Original Sample) With Parameter Tuned\")\n",
        "classifier = LogisticRegression(penalty='none')\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Logistic Classifier (Original Sample) With Parameter Tuned\n",
        "Accuracy : 0.659921798631476\n",
        "Confusion metric : [[483  12]\n",
        " [ 61  32]]\n",
        "f1_Score : 0.4671532846715329\n",
        "Precision : 0.7272727272727273\n",
        "Recall : 0.34408602150537637\n",
        "After Parameter Tuning Our Logistic Regression with origninal sample's accuracy, f1_score is increased by approx 1%.\n",
        "In [179]:\n",
        "# Logistic Classifier (Over Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"Logistic Classifier (Over Sampling)\")\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"Logistic\", 1)\n",
        "========================================\n",
        "Logistic Classifier (Over Sampling)\n",
        "Accuracy : 0.7594982078853046\n",
        "Confusion metric : [[374 121]\n",
        " [ 22  71]]\n",
        "f1_Score : 0.49824561403508766\n",
        "Precision : 0.3697916666666667\n",
        "Recall : 0.7634408602150538\n",
        "{'penalty': 'l2'}\n",
        "In [180]:\n",
        "# Logistic Classifier (Over Sampling) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Logistic Classifier (Over Sampling) With Parameter Tuned\")\n",
        "classifier = LogisticRegression(penalty='l2')\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "========================================\n",
        "Logistic Classifier (Over Sampling) With Parameter Tuned\n",
        "Accuracy : 0.7594982078853046\n",
        "Confusion metric : [[374 121]\n",
        " [ 22  71]]\n",
        "f1_Score : 0.49824561403508766\n",
        "Precision : 0.3697916666666667\n",
        "Recall : 0.7634408602150538\n",
        "In [181]:\n",
        "# Logistic Classifier (Under Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\" Logistic Classifier (Under Sampling)\")\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"Logistic\", 2)\n",
        "========================================\n",
        " Logistic Classifier (Under Sampling)\n",
        "Accuracy : 0.6413489736070381\n",
        "Confusion metric : [[273 222]\n",
        " [ 25  68]]\n",
        "f1_Score : 0.3550913838120104\n",
        "Precision : 0.23448275862068965\n",
        "Recall : 0.7311827956989247\n",
        "{'penalty': 'l2'}\n",
        "In [182]:\n",
        "# Logistic Classifier (Under Sampling) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\" Logistic Classifier (Under Sampling) With Parameter Tuned\")\n",
        "classifier = LogisticRegression(penalty='l2')\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "========================================\n",
        " Logistic Classifier (Under Sampling) With Parameter Tuned\n",
        "Accuracy : 0.6413489736070381\n",
        "Confusion metric : [[273 222]\n",
        " [ 25  68]]\n",
        "f1_Score : 0.3550913838120104\n",
        "Precision : 0.23448275862068965\n",
        "Recall : 0.7311827956989247\n",
        "In [183]:\n",
        "# Naive Bayes Classifier (Original Sample)\n",
        "print(\"=\" * 40)\n",
        "print(\"Naive Bayes Classifier (Original Sample)\")\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "========================================\n",
        "Naive Bayes Classifier (Original Sample)\n",
        "Accuracy : 0.7269468882372108\n",
        "Confusion metric : [[395 100]\n",
        " [ 32  61]]\n",
        "f1_Score : 0.48031496062992124\n",
        "Precision : 0.37888198757763975\n",
        "Recall : 0.6559139784946236\n",
        "In [184]:\n",
        "# Naive Bayes Classifier (Over Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"Naive Bayes Classifier (Over Sampling)\")\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "========================================\n",
        "Naive Bayes Classifier (Over Sampling)\n",
        "Accuracy : 0.683088954056696\n",
        "Confusion metric : [[309 186]\n",
        " [ 24  69]]\n",
        "f1_Score : 0.39655172413793105\n",
        "Precision : 0.27058823529411763\n",
        "Recall : 0.7419354838709677\n",
        "In [185]:\n",
        "# Naive Bayes Classifier (Under Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"Naive Bayes Classifier (Under Sampling)\")\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "========================================\n",
        "Naive Bayes Classifier (Under Sampling)\n",
        "Accuracy : 0.441544477028348\n",
        "Confusion metric : [[171 324]\n",
        " [ 43  50]]\n",
        "f1_Score : 0.21413276231263384\n",
        "Precision : 0.13368983957219252\n",
        "Recall : 0.5376344086021505\n",
        "In [186]:\n",
        "# Decision Tree Classifier (Original Sample)\n",
        "print(\"=\" * 40)\n",
        "print(\"Decision Tree Classifier (Original Sample)\")\n",
        "classifier = DecisionTreeClassifier(random_state=0)\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"Decision\", 0)\n",
        "========================================\n",
        "Decision Tree Classifier (Original Sample)\n",
        "Accuracy : 0.589247311827957\n",
        "Confusion metric : [[429  66]\n",
        " [ 64  29]]\n",
        "f1_Score : 0.30851063829787234\n",
        "Precision : 0.30526315789473685\n",
        "Recall : 0.3118279569892473\n",
        "{'criterion': 'entropy'}\n",
        "In [187]:\n",
        "# Decision Tree Classifier (Original Sample) with parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Decision Tree Classifier (Original Sample) With Parameter Tuned\")\n",
        "classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "========================================\n",
        "Decision Tree Classifier (Original Sample) With Parameter Tuned\n",
        "Accuracy : 0.6316063864450961\n",
        "Confusion metric : [[439  56]\n",
        " [ 58  35]]\n",
        "f1_Score : 0.3804347826086956\n",
        "Precision : 0.38461538461538464\n",
        "Recall : 0.3763440860215054\n",
        "In [188]:\n",
        "# Decision Tree Classifier (Over Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"Decision Tree Classifier (Over Sampling)\")\n",
        "classifier = DecisionTreeClassifier(random_state=0)\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"Decision\", 1)\n",
        "========================================\n",
        "Decision Tree Classifier (Over Sampling)\n",
        "Accuracy : 0.6133919843597263\n",
        "Confusion metric : [[405  90]\n",
        " [ 55  38]]\n",
        "f1_Score : 0.34389140271493207\n",
        "Precision : 0.296875\n",
        "Recall : 0.40860215053763443\n",
        "{'criterion': 'gini'}\n",
        "In [189]:\n",
        "# Decision Tree Classifier (Over Sampling) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Decision Tree Classifier (Over Sampling) With Parameter Tuned\")\n",
        "classifier = DecisionTreeClassifier(random_state=0, criterion='gini')\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "========================================\n",
        "Decision Tree Classifier (Over Sampling) With Parameter Tuned\n",
        "Accuracy : 0.6133919843597263\n",
        "Confusion metric : [[405  90]\n",
        " [ 55  38]]\n",
        "f1_Score : 0.34389140271493207\n",
        "Precision : 0.296875\n",
        "Recall : 0.40860215053763443\n",
        "In [190]:\n",
        "# Decision Tree Classifier (Under Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"Decision Tree Classifier (Under Sampling)\")\n",
        "classifier = DecisionTreeClassifier(random_state=0)\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"Decision\", 2)\n",
        "========================================\n",
        "Decision Tree Classifier (Under Sampling)\n",
        "Accuracy : 0.54613880742913\n",
        "Confusion metric : [[216 279]\n",
        " [ 32  61]]\n",
        "f1_Score : 0.28175519630484985\n",
        "Precision : 0.17941176470588235\n",
        "Recall : 0.6559139784946236\n",
        "{'criterion': 'entropy'}\n",
        "In [191]:\n",
        "# Decision Tree Classifier (Under Sampling) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Decision Tree Classifier (Under Sampling) With Parameter Tuned\")\n",
        "classifier = DecisionTreeClassifier(random_state=0, criterion='entropy')\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "========================================\n",
        "Decision Tree Classifier (Under Sampling) With Parameter Tuned\n",
        "Accuracy : 0.5958944281524927\n",
        "Confusion metric : [[228 267]\n",
        " [ 25  68]]\n",
        "f1_Score : 0.3177570093457944\n",
        "Precision : 0.20298507462686566\n",
        "Recall : 0.7311827956989247\n",
        "In [192]:\n",
        "# Random Forest Classifier (Original Sample)\n",
        "print(\"=\" * 40)\n",
        "print(\"Random Forest Classifier (Original Sample) With Parameter Tuned\")\n",
        "classifier = RandomForestClassifier()\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"Random\", 0)\n",
        "========================================\n",
        "Random Forest Classifier (Original Sample) With Parameter Tuned\n",
        "Accuracy : 0.5873574454219616\n",
        "Confusion metric : [[491   4]\n",
        " [ 76  17]]\n",
        "f1_Score : 0.2982456140350877\n",
        "Precision : 0.8095238095238095\n",
        "Recall : 0.1827956989247312\n",
        "{'criterion': 'gini', 'n_estimators': 200}\n",
        "In [209]:\n",
        "# Random Forest Classifier (Original Sample) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Random Forest Classifier (Original Sample) With Parameter Tuned\")\n",
        "classifier = RandomForestClassifier(criterion='gini', n_estimators=200)\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "========================================\n",
        "Random Forest Classifier (Original Sample) With Parameter Tuned\n",
        "Accuracy : 0.5840013033561421\n",
        "Confusion metric : [[493   2]\n",
        " [ 77  16]]\n",
        "f1_Score : 0.2882882882882883\n",
        "Precision : 0.8888888888888888\n",
        "Recall : 0.17204301075268819\n",
        "In [194]:\n",
        "# Random Forest Classifier (Over Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"Random Forest Classifier (Over Sampling)\")\n",
        "classifier = RandomForestClassifier()\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"Random\", 1)\n",
        "========================================\n",
        "Random Forest Classifier (Over Sampling)\n",
        "Accuracy : 0.6320299771912675\n",
        "Confusion metric : [[482  13]\n",
        " [ 66  27]]\n",
        "f1_Score : 0.406015037593985\n",
        "Precision : 0.675\n",
        "Recall : 0.2903225806451613\n",
        "{'criterion': 'gini', 'n_estimators': 100}\n",
        "In [210]:\n",
        "# Random Forest Classifier (Over Sampling) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Random Forest Classifier (Over Sampling) With Parameter Tuned\")\n",
        "classifier = RandomForestClassifier(criterion='gini', n_estimators=100)\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "========================================\n",
        "Random Forest Classifier (Over Sampling) With Parameter Tuned\n",
        "Accuracy : 0.6202671880091235\n",
        "Confusion metric : [[481  14]\n",
        " [ 68  25]]\n",
        "f1_Score : 0.3787878787878788\n",
        "Precision : 0.6410256410256411\n",
        "Recall : 0.26881720430107525\n",
        "In [196]:\n",
        "# Random Forest Classifier (Under Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"Random Forest Classifier (Under Sampling)\")\n",
        "classifier = RandomForestClassifier()\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"Random\", 2)\n",
        "========================================\n",
        "Random Forest Classifier (Under Sampling)\n",
        "Accuracy : 0.5891169762137504\n",
        "Confusion metric : [[200 295]\n",
        " [ 21  72]]\n",
        "f1_Score : 0.3130434782608696\n",
        "Precision : 0.19618528610354224\n",
        "Recall : 0.7741935483870968\n",
        "{'criterion': 'entropy', 'n_estimators': 100}\n",
        "In [211]:\n",
        "# Random Forest Classifier (Under Sampling) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Random Forest Classifier (Under Sampling) With Parameter Tuned\")\n",
        "classifier = RandomForestClassifier(criterion='entropy', n_estimators=100)\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "========================================\n",
        "Random Forest Classifier (Under Sampling) With Parameter Tuned\n",
        "Accuracy : 0.5992179863147605\n",
        "Confusion metric : [[210 285]\n",
        " [ 21  72]]\n",
        "f1_Score : 0.32000000000000006\n",
        "Precision : 0.20168067226890757\n",
        "Recall : 0.7741935483870968\n",
        "In [198]:\n",
        "# K-NN Classifier (Original Data)\n",
        "print(\"=\" * 40)\n",
        "print(\"K-NN Classifier (Original Data)\")\n",
        "classifier = KNeighborsClassifier()\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"K\", 0)\n",
        "========================================\n",
        "K-NN Classifier (Original Data)\n",
        "Accuracy : 0.5433365917236885\n",
        "Confusion metric : [[490   5]\n",
        " [ 84   9]]\n",
        "f1_Score : 0.16822429906542055\n",
        "Precision : 0.6428571428571429\n",
        "Recall : 0.0967741935483871\n",
        "{'n_neighbors': 3}\n",
        "In [199]:\n",
        "# K-NN Classifier (Original Data) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"K-NN Classifier (Original Data) With Parameter Tuned\")\n",
        "classifier = KNeighborsClassifier(n_neighbors=3)\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "========================================\n",
        "K-NN Classifier (Original Data) With Parameter Tuned\n",
        "Accuracy : 0.559107201042685\n",
        "Confusion metric : [[479  16]\n",
        " [ 79  14]]\n",
        "f1_Score : 0.22764227642276424\n",
        "Precision : 0.4666666666666667\n",
        "Recall : 0.15053763440860216\n",
        "In [200]:\n",
        "# K-NN Classifier (Over Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"K-NN Classifier (Over Sampling) With Parameter Tuned\")\n",
        "classifier = KNeighborsClassifier()\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"K\", 1)\n",
        "========================================\n",
        "K-NN Classifier (Over Sampling) With Parameter Tuned\n",
        "Accuracy : 0.648159009449332\n",
        "Confusion metric : [[317 178]\n",
        " [ 32  61]]\n",
        "f1_Score : 0.3674698795180723\n",
        "Precision : 0.25523012552301255\n",
        "Recall : 0.6559139784946236\n",
        "{'n_neighbors': 3}\n",
        "In [201]:\n",
        "# K-NN Classifier (Over Sampling) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"K-NN Classifier (Over Sampling) With Parameter Tuned\")\n",
        "classifier = KNeighborsClassifier(n_neighbors=3)\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "========================================\n",
        "K-NN Classifier (Over Sampling) With Parameter Tuned\n",
        "Accuracy : 0.6361355490387748\n",
        "Confusion metric : [[353 142]\n",
        " [ 41  52]]\n",
        "f1_Score : 0.3623693379790941\n",
        "Precision : 0.26804123711340205\n",
        "Recall : 0.5591397849462365\n",
        "In [202]:\n",
        "# K-NN Classifier (Under Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"K-NN Classifier (Under Sampling)\")\n",
        "classifier = KNeighborsClassifier()\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"K\", 2)\n",
        "========================================\n",
        "K-NN Classifier (Under Sampling)\n",
        "Accuracy : 0.5750733137829912\n",
        "Confusion metric : [[399  96]\n",
        " [ 61  32]]\n",
        "f1_Score : 0.2895927601809955\n",
        "Precision : 0.25\n",
        "Recall : 0.34408602150537637\n",
        "{'n_neighbors': 5}\n",
        "In [203]:\n",
        "# Support Vector Classifier (Original Sample)\n",
        "print(\"=\" * 40)\n",
        "print(\"Support Vector Classifier (Original Sample)\")\n",
        "classifier = SVC(kernel='rbf')\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"SVM\", 0)\n",
        "========================================\n",
        "Support Vector Classifier (Original Sample)\n",
        "Accuracy : 0.5722385141739981\n",
        "Confusion metric : [[492   3]\n",
        " [ 79  14]]\n",
        "f1_Score : 0.2545454545454546\n",
        "Precision : 0.8235294117647058\n",
        "Recall : 0.15053763440860216\n",
        "{'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
        "In [204]:\n",
        "# Support Vector Classifier (Original Sample) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Support Vector Classifier (Original Sample) With Parameter Tuned\")\n",
        "classifier = SVC(kernel='rbf', C=1000, gamma=0.01)\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "========================================\n",
        "Support Vector Classifier (Original Sample) With Parameter Tuned\n",
        "Accuracy : 0.6779732811990877\n",
        "Confusion metric : [[437  58]\n",
        " [ 49  44]]\n",
        "f1_Score : 0.45128205128205134\n",
        "Precision : 0.43137254901960786\n",
        "Recall : 0.4731182795698925\n",
        "In [205]:\n",
        "# Support Vector Classifier (Over Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"Support Vector Classifier (Over Sampling)\")\n",
        "classifier = SVC(kernel='rbf')\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"SVM\", 1)\n",
        "========================================\n",
        "Support Vector Classifier (Over Sampling)\n",
        "Accuracy : 0.6911045943304008\n",
        "Confusion metric : [[450  45]\n",
        " [ 49  44]]\n",
        "f1_Score : 0.4835164835164835\n",
        "Precision : 0.4943820224719101\n",
        "Recall : 0.4731182795698925\n",
        "{'C': 10, 'gamma': 0.2, 'kernel': 'rbf'}\n",
        "In [206]:\n",
        "# Support Vector Classifier (Over Sampling) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Support Vector Classifier (Over Sampling) With Parameter Tuned\")\n",
        "classifier = SVC(kernel='rbf', C=10, gamma=0.1)\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "========================================\n",
        "Support Vector Classifier (Over Sampling) With Parameter Tuned\n",
        "Accuracy : 0.5372759856630824\n",
        "Confusion metric : [[484  11]\n",
        " [ 84   9]]\n",
        "f1_Score : 0.1592920353982301\n",
        "Precision : 0.45\n",
        "Recall : 0.0967741935483871\n",
        "In [207]:\n",
        "# Support Vector Classifier (Under Sampling)\n",
        "print(\"=\" * 40)\n",
        "print(\"Support Vector Classifier (Under Sampling)\")\n",
        "classifier = SVC(kernel='rbf')\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "Scores_And_GridSearch(\"SVM\", 2)\n",
        "========================================\n",
        "Support Vector Classifier (Under Sampling)\n",
        "Accuracy : 0.5904529162593679\n",
        "Confusion metric : [[196 299]\n",
        " [ 20  73]]\n",
        "f1_Score : 0.3139784946236559\n",
        "Precision : 0.19623655913978494\n",
        "Recall : 0.7849462365591398\n",
        "{'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
        "In [208]:\n",
        "# Support Vector Classifier (Under Sampling) With Parameter Tuned\n",
        "print(\"=\" * 40)\n",
        "print(\"Support Vector Classifier (Under Sampling) With Parameter Tuned\")\n",
        "classifier = SVC(kernel='rbf', C=1, gamma=0.4)\n",
        "classifier.fit(X_train_Under, y_train_Under)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "========================================\n",
        "Support Vector Classifier (Under Sampling) With Parameter Tuned\n",
        "Accuracy : 0.5077875529488433\n",
        "Confusion metric : [[ 29 466]\n",
        " [  4  89]]\n",
        "f1_Score : 0.27469135802469136\n",
        "Precision : 0.16036036036036036\n",
        "Recall : 0.956989247311828\n",
        "After Performing parameter tuning, over-sampling and under-sampling we came to a conclusing for choosing a model with good recall_score and a good balanced_accuracy score. \n",
        "We are choosing balanced accuracy score because it is a measure of recall of positive class + recall of negative class and it outperforms f1_score when positives >> negatives \n",
        "We according to the results got, the best model is Logistic Classifier with oversampling \n",
        "Cause we are getting a good balanced accuracy around 76%\n",
        "And a recall about 76.5%\n",
        "In [212]:\n",
        "print(\"=\" * 40)\n",
        "print(\"Logistic Classifier\")\n",
        "classifier = LogisticRegression(penalty='l2')\n",
        "classifier.fit(X_train_Over, y_train_Over)\n",
        "predictions = classifier.predict(X_test)\n",
        "score_calculator()\n",
        "========================================\n",
        "Logistic Classifier (Over Sampling) With Parameter Tuned\n",
        "Accuracy : 0.7594982078853046\n",
        "Confusion metric : [[374 121]\n",
        " [ 22  71]]\n",
        "f1_Score : 0.49824561403508766\n",
        "Precision : 0.3697916666666667\n",
        "Recall : 0.7634408602150538"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}